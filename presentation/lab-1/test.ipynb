{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Задачей даннной работы является выявления преимуществ и недостатков следующих методов оптимизации нулевого и первого порядка:\n",
    "Замечание:Описание метода предсатвляет сообой переход метода от состояния a к состоинию b ввиду итеративности всех представленных методов \n",
    "1) Градиентный спуск с фиксированным шагом. Описание метода: По исходной точке a и значению градиента функции в ней вычисляется точка b как a - grad(f) * learning_rate, где learning_rate - предварительно заданное значение, неизменное на протяжении всей \n",
    "2) Градиентный спуск с изменяющимся шагом на основе метода дихотомии. Описание метода:  По исходной точке a и значению градиента функции в ней вычисляется точка b как a - grad(f) * learning_rate1, где learning_rate - результат работы метода дихотомии на функции f(learning_rate1) = a - grad(f) * learning_rate1. Левой границей метода служит 0, правой -learning_rate, задаваемый для градиентного спуска. Вычисление останавливается при достижении заданной точности \n",
    "3) Градиентный спуск с изменяющимся шагом на основе метода золотого сечения. Описание метода: аналогичен п.2, за исключением того, что вместо метода дихотомии используется метод золотого сечения\n",
    "Замечание: для большей стабильности методов 1-3 градиент (в формуле вычисления следующего значения) нормируется\n",
    "4) Метод покоординатного спуска. Описание метода: Основываясь на исходной точке x и длине шага step_len Итеративно проверяется точка, получаемая из x прибавлением step_len к проверяемой координате. Если значение меньше, чем в x - происходит переход в данную точку, иначе проверяется значение в точке, получаемой из x вычитанием step_len из проверяемой координаты. Если значение в ней меньше, чем в x - происходит переход, иначе аналогичные проверки проводятся по следующей координате. Если перехода не произошло после проверки всех координат, значение step_len уменьшается\n",
    "5) Метод Нелдера-Мида. Описание метода:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea626ab34b6e2315"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from src.metric import CallCount, GradientCount, StepCountBeforePrecision, TrueGradientCount\n",
    "from src.common.oracul import LambdaOracul, PoweredSumOracul, GradientLambdaOracul\n",
    "\n",
    "from src.lab1.stop_condition import CountCondition, PrecisionCondition, PrecisionOrCountCondition\n",
    "import numpy as np\n",
    "from src.common import Oracul, Point\n",
    "from src.lab1.method_processor import MethodProcessor\n",
    "from src.lab1.methods import RandomMethod, GoldenRatioMethod, GradientDescent, BaseGradientDescent, DichotomyMethod, \\\n",
    "    CoordinateDescent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "method = RandomMethod()\n",
    "golden_ratio = GoldenRatioMethod()\n",
    "gradient_descent_fix = BaseGradientDescent()\n",
    "gradient_descent_golden = GradientDescent()\n",
    "gradient_descent_dichotomy = GradientDescent(method=DichotomyMethod())\n",
    "coordinate_descent = CoordinateDescent()\n",
    "oracul = LambdaOracul(lambda x: (x - 10) ** 2)\n",
    "test_oracul = PoweredSumOracul([[2, 2], [0, 2]])\n",
    "gradient_oracul = GradientLambdaOracul(lambda x, y: (x - 10) ** 2 + (y - 5) ** 2,\n",
    "                                       lambda x, y: np.array([2 * (x - 10), 2 * (y - 5)], dtype=np.float64))\n",
    "point, metrics, anim = MethodProcessor.process(gradient_descent_golden, gradient_oracul, CountCondition(20),\n",
    "                                               metrics=[CallCount()],\n",
    "                                               method_params={\"x\": np.array([100, 200]), 'learning_rate': 100},\n",
    "                                               visualize=True,\n",
    "                                               low_bracket=[-100, -100], high_bracket=[200, 200])\n",
    "print(metrics)\n",
    "print(point)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "cbfe57ebb4981178",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd304002ea7d8b5a",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "base_test = GradientLambdaOracul(lambda x, y: x ** 2 + y ** 2,\n",
    "                                 lambda x, y: np.array([2 * x, 2 * y], dtype=np.float64))\n",
    "base_differential = GradientLambdaOracul(lambda x, y: 0.26 * (x ** 2 + y ** 2) - 0.48 * x * y,\n",
    "                                         lambda x, y: np.array([0.52 * x - 0.48 * y, 0.52 * y - 0.48 * x],\n",
    "                                                               dtype=np.float64))\n",
    "bad_defined_base = GradientLambdaOracul(lambda x, y: 100 * x ** 2 + y ** 2,\n",
    "                                        lambda x, y: np.array([200 * x, 2 * y], dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7db6fdae0a8e84",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def tester(methods, oraculs, eps=0.001, start_point=np.array([100, 200]), visualize=True, braket_l=None, braket_h=None,\n",
    "           learning_rate=300):\n",
    "    res_points = []\n",
    "    res_metrics = []\n",
    "    part_anim_res = []\n",
    "    for m in methods:\n",
    "        part_point_res = []\n",
    "        part_metric_res = []\n",
    "        for f in oraculs:\n",
    "            temp_point, temp_metrics, temp_anim = MethodProcessor.process(m, f, PrecisionCondition(eps),\n",
    "                                                                          metrics=[CallCount(), GradientCount(),\n",
    "                                                                                   StepCountBeforePrecision(eps)],\n",
    "                                                                          method_params={\"x\": start_point,\n",
    "                                                                                         \"learning_rate\": learning_rate,\n",
    "                                                                                         \"eps\": eps},\n",
    "                                                                          visualize=visualize,\n",
    "                                                                          low_bracket=braket_l,\n",
    "                                                                          high_bracket=braket_h)\n",
    "            part_anim_res += [temp_anim]\n",
    "            part_point_res += [temp_point]\n",
    "            part_metric_res += [temp_metrics]\n",
    "        res_points += [part_point_res]\n",
    "        res_metrics += [part_metric_res]\n",
    "    return res_points, res_metrics, part_anim_res\n",
    "\n",
    "\n",
    "def test_fix_grad(oraculs, params, eps=0.001, start_point=np.array([100, 200]), visualize=False, braket_l=None,\n",
    "                  braket_h=None):\n",
    "    res_points = []\n",
    "    res_metrics = []\n",
    "    anim_res = []\n",
    "    for f in range(len(oraculs)):\n",
    "        temp_point, temp_metrics, temp_anim = MethodProcessor.process(BaseGradientDescent(), oraculs[f],\n",
    "                                                                      PrecisionOrCountCondition(eps, 10 ** 5),\n",
    "                                                                      metrics=[CallCount(), GradientCount(),\n",
    "                                                                               StepCountBeforePrecision(eps),\n",
    "                                                                               TrueGradientCount(\n",
    "                                                                                   np.array([0, 0], dtype=np.float64),\n",
    "                                                                                   eps)],\n",
    "                                                                      method_params={\"x\": start_point,\n",
    "                                                                                     \"learning_rate\": params[f],\n",
    "                                                                                     \"eps\": eps},\n",
    "                                                                      visualize=visualize,\n",
    "                                                                      low_bracket=braket_l,\n",
    "                                                                      high_bracket=braket_h)\n",
    "        print(temp_point)\n",
    "        res_points += [temp_point]\n",
    "        res_metrics += [temp_metrics]\n",
    "        anim_res += [temp_anim]\n",
    "    return res_points, res_metrics, anim_res\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tester([gradient_descent_golden, gradient_descent_dichotomy, coordinate_descent],\n",
    "       [base_test, base_differential, bad_defined_base], visualize=False)  #turn on visualization  to demonstration"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "797e0d6d0e070a1b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#test_fix_grad([base_test, base_differential, bad_defined_base], [0.02, 0.02001, 0.0201], visualize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "688bb199a4ac2276",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Замечание: Здесь и далее под методами градиентного спуска понимаются методы на основе градиентного спуска с нефиксированным шагом\n",
    "Первый тест позволяет обнаружить следубщие особенности методов:\n",
    "1) Как наиболее явное отличие, методы покоординатного спуска и Нелдера-Мида не требуют вычисления градиента, что является весьма значительной особенностью и выгодно выделяет их на фоне методов градиетного спуска\n",
    "2) Метод градиетного спуска с фиксированным шагом, хотя и требует вычисления значений функции при данной постановке задачи, не столь сильно в ней нуждается, используя лишь для проверки приближения (как можно заметить, число шагов метода равно числу вычислений значений). Однако едва ли данное преимущество способно перекрыть такие недостатки метода, как отсутствие гарнтии сходимости и потребность в тонкой настройке параметров для обеспечения приемлимой точности или минимальной работоспособности, кроме того число вычислений градиента отличается на порядки ввиду необходимости выставления достаточно малого шага для получения приемлимой точности, кроме того возможны попадания в циклы\n",
    "3) Хотя методы градиентного спуска и требуют несколько большего числа вычислений значений функции (в основном из-за необходимости вычисления шага с достаточной точностью), число их итераций для достижения искомой точности меньше, чем требуемое для покоординатного спуска *предположительно и Нелдера-Мида*\n",
    "4) Метод градиентного спуска на основе дихотомии требует значительно большего числа вычисления значений функций, что является ожидаемым следствием меньшей вычислительной эффективности внутреннего метода"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aabc79681c47d435"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Точность 0.01"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5742a2fd5b9eb170"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tester([GradientDescent(method=GoldenRatioMethod(), aprox_dec=0.001),\n",
    "        GradientDescent(method=DichotomyMethod(), aprox_dec=0.001), coordinate_descent],\n",
    "       [base_test, base_differential, bad_defined_base], eps=0.01, visualize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c417acadf05f775c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#test_fix_grad([base_test, base_differential, bad_defined_base], [0.05, 0.05001, 0.0501], eps=0.01, visualize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "94a51c730e039453",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Точность 0.1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63a14b1153584e9c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tester([GradientDescent(method=GoldenRatioMethod(), aprox_dec=0.01),\n",
    "        GradientDescent(method=DichotomyMethod(), aprox_dec=0.01), coordinate_descent],\n",
    "       [base_test, base_differential, bad_defined_base], eps=0.1, visualize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f11ca3cb6484d544",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#test_fix_grad([base_test, base_differential, bad_defined_base], [0.101, 0.1001, 0.101], eps=0.1, visualize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "cdc70ba8618bfdc0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Таким образом, при понижении точности, общие тенденции сохраняются, хотя и уменьшается разница в абсолютных значениях.\n",
    "Тем не менее, стоит обратить внимание на следующие особенности:\n",
    " 1) понижение точности для градиентных методов означает не только уменьшение числа вычислений функции, но и её градиента\n",
    "2) Благодаря особенностям работы методов, методы градиентного спуска, как правило, вычисляют значение с некоторым запасом точности, в отличие от метода покоординатного спуска\n",
    "3) Хотя запрашиваемое приближение для внутренних методов градиентых спусков (дихотомии и золотого сечения) одинаково, можно заметить, что в двух тестах число итераций градиентного спуска на основе метода золотого сечения было меньшим. Вероятно, данная особенность связана с попаданием минимума в меньший из промежутков и, как следствие, вычисление с большей реальной точностью, при меньшем объёме вычислений.\n",
    "4) Использование метода градиентного спуска с фиксированным шагом на функции неизвестного вида с целью получения заданного приближения видится сомнительным\n",
    "5) При уменьшении запрашиваемой точности применение метода градиентного спуска с фиксированным шагом видится более целесообразным, нежели при желании получить высокую точность, ведь хотя метод и входил в бесконечный цикл, будучи неспособным приблизиться к точке или понять ,что он уже достаточно близок, приближение происходило за разумное время"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29e60141708143a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Изменённая точка старта"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7de7e97a42900fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tester([gradient_descent_golden, gradient_descent_dichotomy, coordinate_descent],\n",
    "       [base_test, base_differential, bad_defined_base], start_point=np.array([10, 20]), visualize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "53e04eccfb9e9ab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tester([gradient_descent_golden, gradient_descent_dichotomy, coordinate_descent],\n",
    "       [base_test, base_differential, bad_defined_base], start_point=np.array([-100, -200]), visualize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "33a046f31dc084a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Влияение выбора точки на работу метдов замечено не было.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c2ab56f0a660f4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bad_defined = GradientLambdaOracul(lambda x, y: 1000 * (np.float64(x) - 10) ** 10 + 100 * (np.float64(y) + 20) ** 10,\n",
    "                                   lambda x, y: np.array(\n",
    "                                       [1000 * (np.float64(x) - 10) ** 9, 1000 * (np.float64(y) + 20) ** 9],\n",
    "                                       dtype=np.float64))\n",
    "tester([gradient_descent_golden, gradient_descent_dichotomy, coordinate_descent],\n",
    "       [bad_defined], start_point=np.array([-100, -200]), eps=0.001, visualize=True, braket_h=[100, 100],\n",
    "       braket_l=[-100, -100], learning_rate=10)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "33c557ab604e9295",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#test_fix_grad([bad_defined], [1], eps=0.01, visualize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5247538bd779bf96",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Таким образом, все методы справляются с выполнением своих задач на плохо обусловленных функциях(по крайней мере, до тех пор, пока их значчения представляется возможным рассчитать), хотя и наиболее эффективен здесь метод покоординатного спуска. Т.к. значение функции меняется весьма значительно даже при небольшом отклонении, методам шрадиентного спуска приходится запрашивать значительное число значений. Так же примечательно то, что метод координатного спуска сделал меньшее число итераций"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c957eeab66547b58"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "isom_function = GradientLambdaOracul(\n",
    "    lambda x, y: -np.cos(x) * np.cos(y) * np.exp(-((x - np.pi) ** 2 + (y - np.pi) ** 2)),\n",
    "    lambda x, y: np.array(\n",
    "        [np.exp(-(x - np.pi) ** 2 - (y - np.pi) ** 2) * np.cos(x) * np.cos(y) + 2 * (x - np.pi) * np.cos(x),\n",
    "         np.exp(-(x - np.pi) ** 2 - (y - np.pi) ** 2) * np.cos(x) * np.cos(y) + 2 * (y - np.pi) * np.cos(y)],\n",
    "        dtype=np.float64))\n",
    "base_mulltimodal = GradientLambdaOracul(lambda x, y: (x ** 2 + y - 11) ** 2 + (x + y ** 2 - 7) ** 2,\n",
    "                                        lambda x, y: np.array([2 * (2 * x * (x ** 2 + y - 11) + x + y ** 2 - 7),\n",
    "                                                               2 * (x ** 2 + 2 * y * (x + y ** 2 - 7) + y - 11)],\n",
    "                                                              dtype=np.float64))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3f689e597c7b2bd2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ec9a53d2348290f7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tester([gradient_descent_golden, gradient_descent_dichotomy, coordinate_descent],\n",
    "       [base_mulltimodal], start_point=np.array([-1000, -2000]), eps=0.0001, visualize=True, braket_h=[1000, 1000],\n",
    "       braket_l=[-1000, -1000], learning_rate=10)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "416baec46b4c7e0c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ожидаемым образом, функции сходятся к оптимумам. В случае с base_multimodal - к глобальным (т.к. все являются таковыми), хотя и примечательно, что покоординатный спуск сошёлся к отличному от градиентных"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3485cb63783d3c8b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "10976add5afea818"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3cfbb82136b0dfa4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
