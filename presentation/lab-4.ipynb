{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa9838a5a819a7",
   "metadata": {},
   "source": [
    "# Методы стохастической оптимизации. Настройка гиперпараметров\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import random\n",
    "import time\n",
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.rcParams['figure.dpi'] = 200\n",
    "pyplot.rcParams['savefig.dpi'] = 200\n",
    "from sys import platform\n",
    "import sys\n",
    "\n",
    "if platform == \"linux\" or platform == \"linux2\" or platform == \"darwin\":\n",
    "    sys.path.append(\"../../\")\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "from src.v2.impl.conditions import StepCountCondition, PrecisionCondition, AbsolutePrecisionCondition\n",
    "from src.v2.impl.methods import CoordinateDescent\n",
    "from src.v2.impl.metrics import StepCount, CallCount, GradientCallCount, HessianCallCount, ResultValue, RAMSize, ExecutionTime\n",
    "from src.v2.impl.oraculs import LambdaOracul\n",
    "from src.v2.runner.runner import Runner, FULL_VISUALIZE, VISUALIZE\n",
    "from src.v2.visualization.animation import Animator\n",
    "from src.v2.runner.runner import TABLE\n",
    "from src.v2.impl.methods import GradientDescent, ScipyMethod, SimulatedAnnealing, Evolution, EvolutionRecombination, EvolutionCMA\n",
    "from src.v2.impl.schedules import step_schedule, pow_step_schedule\n",
    "from IPython.display import display, HTML\n",
    "from math import exp, sqrt, cos, sin, pi, e\n",
    "\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "\n",
    "def seed(val: int = -1):\n",
    "    if val == -1:\n",
    "        val = time.time_ns()\n",
    "    random.seed(val)\n",
    "    np.random.seed(val % 2 ** 32)\n",
    "    print(\"SEED:\", val)\n",
    "\n",
    "\n",
    "modules = [ExecutionTime(), RAMSize(), Animator(), StepCount(), CallCount(), GradientCallCount(), HessianCallCount(),\n",
    "           ResultValue()]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41e14ff1738c9b7b",
   "metadata": {},
   "source": [
    "# Постановка задачи\n",
    "Главной задачей нашей работы является изучение теоретической и практической частей стахастических методов оптимизации. Для каждого метода будет представлено теоретическое обоснавание работоспособности метода. Чтобы определить эффективность методов в практическом применении будут рассмотрены задачи из предыдущих лабораторных работ, а также будет проведено сравнение различных метрик как и между стахастическими методами, так и с методами из предыдущих лабораторных работ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1e6b925fcb357",
   "metadata": {},
   "source": [
    "# Основное задание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6947e6e842587",
   "metadata": {},
   "source": [
    "### Метог отжига\n",
    "Первым методом, который будет предложен для изучения - **метод имитации отжига**. Как понятно из названия, данный метод основывается на имитации физического процесса. Суть метода заключается в случайной генерации следующего состояния системы на основе предыдущего. Если состояние оказывается лучше текущего, то совершается переход, если хуже, то переход совершается с каким-то шансом, зависящим от номера итерации. Обычно шанс на переход в худшее состояние имеет линейную убывающую зависимость, сами параметры очень сильно влияют на поведение метода, поэтому подбираются под каждую задачу отдельно.\n",
    "\n",
    "Главное преимущество такого алгоритма - возможность выбираться из локальных минимумов, так как оказавшись в таком есть шанс несколько раз подряд оказаться в худшем состоянии и оказаться в области, откуда можно добраться до глобального минимума или другого локального. Сначала можно подумать, что тогда метод будет вести себя очень случайно и даже оказавшись возле глобального минимума будет иметь возможность выйти оттуда. Но если вспомнить что шанс на переход в худшее состояние - убывающая функция, то предположив, что метод постепенно приближался к глобальному минимуму, то уменьшив свой шанс на переход в худшее состояние до необходимого значения, он перекроет себе возможность на возвращение в те области из которых он пришёл и чьё состояние хуже текущей области."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ee91826455cf",
   "metadata": {},
   "source": [
    "В нашей реализации метод параметризуется с помощью **learning rate**, максимального отклонения от предыдущего состояния по каждой из осей, и **decay**, значения на которое уменьшается ганс на переход в худщее состояние после каждой итерации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6348611a48f55f9d",
   "metadata": {},
   "source": [
    "Чтобы сравнить метод отжига с другими методами рассмотрим авторскую двумерную функцию с большим количеством локальных минимумов. В качестве методов для сравнения будут выступать покоординатный спуск и Ньютоновский метод из библиотеки scipy."
   ]
  },
  {
   "cell_type": "code",
   "id": "fb54bf8ce2d9468d",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "seed(1717697282242590600)\n",
    "\n",
    "methods = [\n",
    "    SimulatedAnnealing(3, 0.7, 0.01),\n",
    "    CoordinateDescent(learning_rate=3, aprox_dec=1e-30),\n",
    "    ScipyMethod(\"Newton-CG\")\n",
    "]\n",
    "conditions = [\n",
    "    StepCountCondition(100),\n",
    "    AbsolutePrecisionCondition(1e-5, np.array([0]))\n",
    "]\n",
    "\n",
    "hard_function = LambdaOracul(\n",
    "    lambda x: -5 * exp(cos(x) ** 2 - abs(x) / 6) + abs(x) / 5 * (sin(1.5 * x) * cos(2 * x / 3)) ** 2 + abs(sin(10 * x)))\n",
    "\n",
    "result = Runner.run(methods, [hard_function], np.array([9.0]), conditions + modules, precision=1e-7,\n",
    "                    **TABLE, **VISUALIZE, animate=True, animate_main=True, animation_main_full=True,\n",
    "                    animator_main_step=0.01)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "66ca88aeea1ec444",
   "metadata": {},
   "source": [
    "В данном тесте метрики занимаемой памяти и времени исполнения немного подвели, так как и имитация отжига и координатный спуск отрабатывают так быстро, что сборщик мусора не успевает очищать память. Но не смотря на это видно превосходство метода имитации отжига - это единственный метод, который сошелся. Но из анализа результатов и наблюдений во время проведения тестов можно сделать вывод о невозможности достижения высокой точности этим методом в чистом виде. Так как область в которой случайным образом выбирается следующее состояние статична, то оказавшись возле минимума метод может лишь надеяться на удачу и пытаться оказаться ближе к минимуму. Помочь с этой проблемой может schedule, то есть понижание learning rate с увеличением числа итераций."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5372bbedcd6be2d",
   "metadata": {},
   "source": [
    "Чтобы определить эффективность добавления schedule для метода имитации отжига, предлагается провести сравнение на той же функции. В качестве schedule функции будет выступать алгоритм уменьшения learning rate в зависимости от количества числа итераций из предыдущей лабораторной работы, но с модификацией - теперь число итераций может возводиться в произвольную степень. Такая функция позволяет выставив очень маленький коэффициент и неединичную степень и таким образом почти не воздействовать на learning rate на первых итерациях. Например в нашем случае выбраны коэффициент 1e-6 и степень 3, значит на 100 шагу learning rate уменьшится всего в 2 раза, а на 500 шагу в 126 раз."
   ]
  },
  {
   "cell_type": "code",
   "id": "df6d3ba400de3516",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "seed(1717700296282255800)\n",
    "\n",
    "methods = [\n",
    "    SimulatedAnnealing(3, 0.7, 0.01),\n",
    "    pow_step_schedule(3.5, 0.000001, 3, SimulatedAnnealing)(3.5, 0.7, 0.01)\n",
    "]\n",
    "\n",
    "conditions = [\n",
    "    StepCountCondition(500)\n",
    "]\n",
    "\n",
    "result = Runner.run(methods, [hard_function], np.array([9.0]), conditions + modules, precision=1e-7,\n",
    "                    **TABLE, **VISUALIZE, animate=True, animate_main=True, animation_main_full=True,\n",
    "                    animator_main_step=0.01)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "18c238885160eeb1",
   "metadata": {},
   "source": [
    "По результатам видно, что schedule версия достигла большей точности за то же число итераций. Значит применение schedule более чем осмыслено, поэтому в дальнейшем рассмотрении будет использоваться именно schedule версия."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988222a747ecce26",
   "metadata": {},
   "source": [
    "Теперь рассмотрим метод имитации отжига на какой-нибудь функции, на которой легко сойтись, чтобы проверить эффектвность такого метода в тех ситуациях, когда классические методы показывают себя неплохо."
   ]
  },
  {
   "cell_type": "code",
   "id": "8a2f3105b1bb5547",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "seed(1717701126228002400)\n",
    "\n",
    "methods = [\n",
    "    pow_step_schedule(5, 0.0001, 4, SimulatedAnnealing)(5, 0.3, 0.05),\n",
    "    CoordinateDescent(learning_rate=3, aprox_dec=1e-30),\n",
    "    ScipyMethod(\"Newton-CG\")\n",
    "]\n",
    "conditions = [\n",
    "    StepCountCondition(100),\n",
    "    AbsolutePrecisionCondition(1e-7, np.array([0]))\n",
    "]\n",
    "\n",
    "easy_function = LambdaOracul(lambda x: x**2)\n",
    "\n",
    "result = Runner.run(methods, [easy_function], np.array([9.0]), conditions + modules, precision=1e-7,\n",
    "                    **TABLE, **VISUALIZE, animate=True, animate_main=True, animation_main_full=True,\n",
    "                    animator_main_step=0.01)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa5b84cd039b198",
   "metadata": {},
   "source": [
    "Метод имитации отжига сходился медленее и менее точно чем остальные, что не удивительно зная суть данного алгоритма. На самом деле разница просто огромна, пока классическимЪ методами нужна всего пара итераций чтобы сойтись с невероятной точностью, методу имитации отжига нужны сотни итераций и это с учётом schedule, который тоже требовал первоночального подгона и который нехило увеличил точность по сравнению с обычным результатом. Исходя из вышесказанного можно сделать вывод, что использование метода имитации обжига на унимодальных функциях нецелесообразно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6901a470d2818060",
   "metadata": {},
   "source": [
    "## Эволюционные методы оптимизации\n",
    "Недостаток метода имитации отжига виден на лицо, но можно ли его как-то исправить? Если каждая итерация этого метода такая легковесная, то может просто увеличить их количество? Логическим продолжением метода отжига можно назвать эволюционные алгоритмы. Основная суть таких алгоритмов - генерация потомков из родителей и использование лучших из представителей для следующего поколения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f561b4fcb0447",
   "metadata": {},
   "source": [
    "### (μ,λ)-ES и (μ+λ)-ES\n",
    "Сначала рассмотрим стандартные эволюционные методы: **(μ,λ)-ES** и **(μ+λ)-ES**. \n",
    "Алгоритм этих методов очень прост:\n",
    "1. Случайно относительно родителей генерируются потомки.\n",
    "2. Те представители, которые оказались лучше по характеристикам (в нашем случае, где значение оракула наименьшее) становятся родителями для следующего поколения.\n",
    "3. Повторяем\n",
    "\n",
    "Отличие методов заключается лишь в том, что первый метод использует только потомков в следуюшем поколении, а второй и потомков и родителей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c075afcc3e6dce",
   "metadata": {},
   "source": [
    "В нашем случае они реализованы через один класс, имеющим такой алгоритм: На первом шагу случайно относительно стартовой точки генерируется необходимое количество родителей. На следующих шагах каждый ребенок генерируется относительно случайного родителя, среди всех потомков и какого-то числа лучших представителей родителей (данное число задано параметром, что и позволило через один алгоритм реализовать оба метода: в одном случае этот параметр равен 0, в другом числу родителей) выбираются лучшие, они станут родителями для следующего поколения. Точкой которая вовзращается на итерации - лучшая точка среди тех, что станут родителями на следующей итерации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d116ab21eaa8a",
   "metadata": {},
   "source": [
    "Методы параметризуются с помощью **learning rate**, максимального отклонения от родителя по каждой из осей, **parents**, количества родиетелей, **childrens**, количества детей, и **reuse parents**, числа лучших представителей родителей, которые могут использоваться в следующей итерации, знаение меньше 0 означает использование всех родителей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0017c675a82c737",
   "metadata": {},
   "source": [
    "Сравним эти методы на сложной функции из первого сравнения:"
   ]
  },
  {
   "cell_type": "code",
   "id": "8ceee7c634da5adc",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "seed(1717703580539391700)\n",
    "\n",
    "methods = [\n",
    "    pow_step_schedule(3.5, 0.000001, 3, SimulatedAnnealing)(3.5, 0.7, 0.02),\n",
    "    Evolution(50, 10, 50, 0),\n",
    "    Evolution(50, 10, 50, 50),\n",
    "    CoordinateDescent(learning_rate=3, aprox_dec=1e-30),\n",
    "]\n",
    "\n",
    "conditions = [\n",
    "    StepCountCondition(500)\n",
    "]\n",
    "\n",
    "result = Runner.run(methods, [hard_function], np.array([9.0]), conditions + modules, precision=1e-7,\n",
    "                    **TABLE, **VISUALIZE, animate=True, animate_main=True, animation_main_full=True,\n",
    "                    animator_main_step=0.01)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bb6599c00c519639",
   "metadata": {},
   "source": [
    "Оба метода показали сходимость к глобальному минимуму, но **(μ+λ)-ES** достиг намного большей точности чем **(μ,λ)-ES** и даже обогнал метод имитации отжига, но кажется что метод может достигнуть большей точности если уменьшать learning rate с помощью schedule, но об этом позже. Превосходство метода, который не отбрасывает родителей очевиден - во-первых забывать о родителях это не по-христиански и бог будет всячески мешать, а во вторых возможна ситуация когда при генерации поколения были выбраны плохие родители или потомки просто сгенерировались неудачно, тогда использование только детей в следующем состоянии может ухудшить текущее положение, но такой подход может позволить вылезти из локальных минимумов. Также не стоит забывать, что итоговая точка - лучшая точка на последней итераци, поэтому возможна ситуация, когда какой-то из родителей уже находится возле глобального минимума, но ни один из потомков не сгенерировался возле него, тогда на следующей итерации об этой точке забудут и в зачёт пойдёт какой-то из менее успешных потомков."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce76e2eb129fbf5",
   "metadata": {},
   "source": [
    "Кажется, что главное преимущество эволюционных алгоритмов - множество 'ветвей' которые могут рассматриваться алгоритмом одновременно, на двумерном пространстве данное преимущество теряется поэтому рассмотрим функцию большей размерности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218636d1b0f1aed",
   "metadata": {},
   "source": [
    "В качестве функции для сравнения рассмотрим функцию леви из 1 лабораторной работы. При проведении эксперимента с этой функции все методы сошлись к локальным минимумам, которые достаточно далеко находились от глобального."
   ]
  },
  {
   "cell_type": "code",
   "id": "b71fc90f57da0c61",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "seed(1717709095419857600)\n",
    "\n",
    "methods = [\n",
    "    pow_step_schedule(70, 0.000001, 2, SimulatedAnnealing)(70, 0.7, 0.02),\n",
    "    Evolution(50, 10, 50, 0),\n",
    "    Evolution(50, 10, 50, -1),\n",
    "    GradientDescent(learning_rate=10, aprox_dec=1e-5),\n",
    "    ScipyMethod(\"Newton-CG\")\n",
    "]\n",
    "conditions = [\n",
    "    StepCountCondition(2000),\n",
    "    AbsolutePrecisionCondition(1e-2, [5, 5])\n",
    "]\n",
    "\n",
    "\n",
    "def levi_function(x, y):\n",
    "    FACTOR = 5\n",
    "    return (np.sin(3 / FACTOR * np.pi * x) ** 2\n",
    "            + (x / FACTOR - 1) ** 2 * (1 + np.sin(3 * np.pi * y / FACTOR) ** 2)\n",
    "            + (y / FACTOR - 1) ** 2 * (1 + np.sin(2 * np.pi * y / FACTOR) ** 2))\n",
    "\n",
    "\n",
    "levi_oracul = LambdaOracul(levi_function)\n",
    "\n",
    "point = np.array([1000.0, 1000.0])\n",
    "result = Runner.run(methods, [levi_oracul], point, modules + conditions, precision=1e-7,\n",
    "                    **TABLE, **VISUALIZE, **FULL_VISUALIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "745f2f45b052bce7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6d7d60d1289f0c9",
   "metadata": {},
   "source": [
    "Классические методы повторили свой результат - они сошлись к локальным минимумам, что ожидаемо, ведь данные методы не гарантируют сходимости на не унимодальных функциях. Стахастические методы показали себя намного лучше - все оказались возле глобального минимума, но **(μ+λ)-ES** показал наивысшую точность.\n",
    "\n",
    "Сходимость методов легко объяснить, если рассмотреть общий вид функции на графиках. Если убрать выступы, то заметно сильное убываение функции по обеим осям в сторону глобального минимума. Стахастическим методам этого достаточно, чтобы с большим шансом двигаться именно в сторону глобального минимума и в итоге оказаться совсем близко к нему. При этом выступы почти никак не мешают методам в случае если learning rate больше ширины выступа, так как методы могут с легкостью перепрыгнуть через них."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93345ca1a505fc31",
   "metadata": {},
   "source": [
    "Теперь хотелось бы рассмотреть эволюционные методы на унимодальных функциях, чтобы узнать, унаследовали ли он главный недостаток метода имитации обжига. В качестве функции для сравнения будет использовться функция Бута из 1 лабораторной работы."
   ]
  },
  {
   "cell_type": "code",
   "id": "b4fb02f87acc6405",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "seed(1717712609050922700)\n",
    "\n",
    "methods = [\n",
    "    pow_step_schedule(70, 0.0001, 2, SimulatedAnnealing)(70, 0.7, 0.02),\n",
    "    Evolution(50, 10, 50, 0),\n",
    "    Evolution(50, 10, 50, -1),\n",
    "    CoordinateDescent(learning_rate=10, aprox_dec=1e-3),\n",
    "    GradientDescent(learning_rate=100, aprox_dec=1e-5),\n",
    "    ScipyMethod(\"Newton-CG\")\n",
    "]\n",
    "conditions = [\n",
    "    StepCountCondition(2000),\n",
    "    AbsolutePrecisionCondition(1e-5, [1, 3])\n",
    "]\n",
    "\n",
    "bute = LambdaOracul(lambda x, y: (x + 2 * y - 7) ** 2 + (2 * x + y - 5) ** 2)\n",
    "\n",
    "result = Runner.run(methods, [bute], np.array([100, 100]), modules + conditions,\n",
    "                    precision=1e-7, **TABLE, **VISUALIZE, **FULL_VISUALIZE)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "96de798274b107e3",
   "metadata": {},
   "source": [
    "Градиентный спуск и Newton-CG ожидаемо сошлись, но к удивлению координатный спуск сошелся в точку [1, 2], хотя минимум был в [1, 3]. Стахастические методы тоже сошлись к глобальному минимуму, причем метод имитации отжига сошелся достаточно близко, в то время как эволюционные методы показали очень низкую точность. Данный недостаток можно решить с помощью schedule, но существует более практичное решение - это логическое продолжение эволюционных методов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e1f7d1a46ce6b6",
   "metadata": {},
   "source": [
    "### CMA-ES\n",
    "Cледующим методом после (μ,λ)-ES и (μ+λ)-ES является **CMA-ES**. Это метод адаптации матриц ковариации. Суть этого метода почти полностью совпадает с вышеописанными эволюционными методами за исключением того, как потомки генерируются из родителей. Для этого на основе родителей находится мат ожидание и матрица ковариации, с помощью которых строится нормальное распределение, которое с наибольшим шансом могло сгенерировать данную конфигурацию точек. С помощью найденного распределения случайным образом генерируются потомки. В остальном метод совпадает с другими эволюционными методами. Главный неочевидный плюс **CMA-ES** заключается в распределении, которое получается на каждой итерации. Если метод уже находится возле глобального минимума и все родители находятся недалеко друг от друга, то генерируется распределение с небольшим разбросом, то есть потомки будут генерироваться в небольшой области. Другими словами, по мере приближения к глобальному минимуму разброс будет уменьшаться и проблема с необходимостью уменьшения learning rate не возникнет."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac04de6b133555",
   "metadata": {},
   "source": [
    "Метод параметризуeтся с помощью **learning rate**, максимального отклонения по каждой из осей от стартовой точки для первого поколения, **parents**, количества родиетелей, **childrens**, количества детей, и **reuse parents**, числа лучших представителей родителей, которые могут использоваться в следующей итерации, знаение меньше 0 означает использование всех родителей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5479678588e6be",
   "metadata": {},
   "source": [
    "Сначала проверим работоспомобность на сложной функции, например на функции Экли."
   ]
  },
  {
   "cell_type": "code",
   "id": "27763f0a-1621-46b0-b87f-bc662359a6e4",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "seed(1717716266320449600)\n",
    "\n",
    "methods = [\n",
    "    pow_step_schedule(70, 0.00001, 2, SimulatedAnnealing)(70, 0.3, 0.005),\n",
    "    Evolution(50, 10, 50, -1),\n",
    "    EvolutionCMA(200, 10, 100, -1),\n",
    "    GradientDescent(learning_rate=50, aprox_dec=1e-5),\n",
    "    ScipyMethod(\"Newton-CG\")\n",
    "]\n",
    "conditions = [\n",
    "    StepCountCondition(2000),\n",
    "    AbsolutePrecisionCondition(1e-5, [0, 0])\n",
    "]\n",
    "\n",
    "ekli = LambdaOracul(\n",
    "    lambda x, y: -20 * exp(-0.2 * sqrt((x ** 2 + y ** 2) / 2)) - exp((cos(2 * pi * x) + cos(2 * pi * y)) / 2) + e + 20)\n",
    "\n",
    "result = Runner.run(methods, [ekli], np.array([100, 100]), modules + conditions, precision=1e-7, **TABLE, **VISUALIZE, **FULL_VISUALIZE)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "748c4ff74f7df0d8",
   "metadata": {},
   "source": [
    "Классические методы ожидаемо не сошлись. Стахастические повели себя по-разному. Метод имитации отжига не сошелся к глобальному минимуму, в отличие от функции леви на данной функции нет заметного спада в сторону глобального минимума, поэтому локальные минимумы почти равносильны для данного метода и схождение к глобальному минимуму может быть не более чем удачей. Эволюционные методы же сошлись, как раз из-за большего количества локальных минимумов, которые может рассматривать метод. (μ+λ)-ES сошелся с низкой точностью и довольно медлено из-за описанных ранее проблем, в то время как CMA-ES сошелся невероятно быстро и невероятно точно. Трудно теоретически объяснить почему функция показала такой отличный результат."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526958ca152601c9",
   "metadata": {},
   "source": [
    "Сравним эффективность данного метода на унимодальной функции, например на функции Химмельблау."
   ]
  },
  {
   "cell_type": "code",
   "id": "a1b0106c-316f-4427-be53-c990865f1c79",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "seed(1717717613926377900)\n",
    "methods = [\n",
    "    pow_step_schedule(70, 0.00001, 2, SimulatedAnnealing)(70, 0.3, 0.005),\n",
    "    Evolution(50, 10, 50, -1),\n",
    "    EvolutionCMA(200, 10, 100, -1),\n",
    "    GradientDescent(learning_rate=50, aprox_dec=1e-5),\n",
    "    ScipyMethod(\"Newton-CG\")\n",
    "]\n",
    "\n",
    "himmelblau = LambdaOracul(lambda x, y: (x ** 2 + y - 11) ** 2 + (x + y ** 2 - 7) ** 2)\n",
    "\n",
    "conditions = [\n",
    "    StepCountCondition(2000),\n",
    "    AbsolutePrecisionCondition(1e-2, [3, 2]),\n",
    "    AbsolutePrecisionCondition(1e-2, [-2.805118, 3.131312]),\n",
    "    AbsolutePrecisionCondition(1e-2, [-3.779310, -3.283186]),\n",
    "    AbsolutePrecisionCondition(1e-2, [3.584428, -1.8481126]),\n",
    "    PrecisionCondition(1e-4)]\n",
    "\n",
    "result = Runner.run(methods, [himmelblau], np.array([100, 100]), modules + conditions, precision=1e-7, **TABLE,\n",
    "                    **VISUALIZE, **FULL_VISUALIZE)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e95b828f9e218f0",
   "metadata": {},
   "source": [
    "Здесь CMA-ES показал себя отлично, всего на пару итераций медленее сойдясь чем метод градиентного спуска. Показав схожие результаты по времени исполнения и требуемой оперативной памяти, он сделал кратно больше вызовов. То есть в случае, если вызов оракула может быть тяжеловестной операцией, как например во многих задачах машиного обучения, то CMA-ES покажет результат намного хуже, но не смотря на это CMA-ES решает намного большее количество задач чем могут решить классические алгоритмы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc7d9e9f87bbf57",
   "metadata": {},
   "source": [
    "## Общее сравнение\n",
    "Определившись с плюсами и минусами описанных методов, хотелось бы провести ещё несколько тестов для подтверждения выдвинутых утверждений, объясняющих поведение методов на тех или иных функциях."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b527d08aa23e655c",
   "metadata": {},
   "source": [
    "Начнём с функции Розенброка, в предыдущих лабараторных работах классические методы показали себя относительной плохо, исключением является Newton-CG, который показал отличные результаты."
   ]
  },
  {
   "cell_type": "code",
   "id": "5c514bcd144fa3",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "seed(1717718715696515100)\n",
    "\n",
    "methods = [\n",
    "    pow_step_schedule(20, 0.00001, 2, SimulatedAnnealing)(70, 0.3, 0.005),\n",
    "    Evolution(10, 10, 50, -1),\n",
    "    EvolutionCMA(100, 10, 100, -1),\n",
    "    GradientDescent(learning_rate=10, aprox_dec=1e-5),\n",
    "    ScipyMethod(\"Newton-CG\")\n",
    "]\n",
    "conditions = [\n",
    "    StepCountCondition(2000),\n",
    "    AbsolutePrecisionCondition(1e-5, [1, 1])\n",
    "]\n",
    "rosenbrok = LambdaOracul(lambda x, y: (1 - np.float64(x)) ** 2 + 100 * (np.float64(y) - np.float64(x) ** 2) ** 2)\n",
    "\n",
    "result = Runner.run(methods, [rosenbrok], np.array([50, -50]), modules + conditions,\n",
    "                    precision=1e-7, **TABLE, **VISUALIZE, **FULL_VISUALIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "31de2247f3f56bf9",
   "metadata": {},
   "source": [
    "Результат эксперимента из предыдущих лабораторных работ повторился - Newton-CG сошелся, градиентный спуск не сошелся, но оказался довольно близко. Стахастические методы показали результат похожий на предыдущие тесты, метод имитации отжига был близок, но не точен и медленен, (μ+λ)-ES был быстрее и точнее, но все равно недостаточно, CMA-ES всего за немного больше десятка операций сошелся с удивительной точностью."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Проверим CMA-ES на функции с которой никто не справлялся - Изома.",
   "id": "7694307cfac48038"
  },
  {
   "cell_type": "code",
   "id": "232cf67f-b248-4a71-ab04-9fd56e718e70",
   "metadata": {},
   "source": [
    "seed(1717719845694252000)\n",
    "\n",
    "methods = [\n",
    "    pow_step_schedule(70, 0.00001, 2, SimulatedAnnealing)(70, 0.3, 0.005),\n",
    "    Evolution(50, 10, 50, -1),\n",
    "    EvolutionCMA(200, 10, 100, -1),\n",
    "    GradientDescent(learning_rate=50, aprox_dec=1e-5),\n",
    "    ScipyMethod(\"Newton-CG\")\n",
    "]\n",
    "conditions = [\n",
    "    StepCountCondition(2000),\n",
    "    AbsolutePrecisionCondition(1e-5, [pi, pi])\n",
    "]\n",
    "\n",
    "isom = LambdaOracul(lambda x, y: -np.cos(x) * np.cos(y) * np.exp(-((x - np.pi) ** 2 + (y - np.pi) ** 2)))\n",
    "\n",
    "result = Runner.run(methods, [isom], np.array([50, 50]), modules + conditions,\n",
    "                    precision=1e-7, **TABLE, **VISUALIZE, **FULL_VISUALIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Здесь сошлись только (μ+λ)-ES и CMA-ES, но второй опять сделал это быстрее и намного точнее.",
   "id": "6a4e3c5d29f55620"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Пришло время для финального босса - авторской функции, которую было решено назвать функцией Скорнякова. Это функция с множеством локальных минимумов, и даже недалеко от глобального минимума есть множество локальных минимумов, которые должны стать ловушкой для стахастических методов.",
   "id": "24c696347d2b5e92"
  },
  {
   "cell_type": "code",
   "id": "f6bcdd16-8da7-47ad-b97a-cb19601a5b72",
   "metadata": {},
   "source": [
    "seed()\n",
    "\n",
    "methods = [\n",
    "    pow_step_schedule(50, 0.00001, 2, SimulatedAnnealing)(70, 0.3, 0.005),\n",
    "    Evolution(10, 10, 50, -1),\n",
    "    EvolutionCMA(200, 10, 100, -1),\n",
    "    GradientDescent(learning_rate=50, aprox_dec=1e-5),\n",
    "    ScipyMethod(\"Newton-CG\")\n",
    "]\n",
    "conditions = [\n",
    "    StepCountCondition(2000),\n",
    "    AbsolutePrecisionCondition(1e-5, [0, 0])\n",
    "]\n",
    "\n",
    "skornyakov_function = LambdaOracul(lambda x, y: sqrt((x + 4 * sin(x)) ** 2 + (y + 2 * sin(y)) ** 2))\n",
    "\n",
    "result = Runner.run(methods, [skornyakov_function], np.array([50, -50]), modules + conditions, precision=1e-7, **TABLE, **VISUALIZE,**FULL_VISUALIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Результат не удивил, классические функции разумеется не сошлись. Метод имитации отжига попал в один из локальных минимумов из которого не смог вылезти, поэтому он не сошелся. Эволюционные методы сошлись и опять CMA-ES сделал это очень быстро и с высокой точностью.",
   "id": "de50a2f4c9bbf58a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
